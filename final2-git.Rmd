---
title: "Final project of BST 260 -- Analysis of Partial Nephrectomy"
output: github_document
date: "2022-12-14"
---
**Introduction**  
Partial nephrectomy is a technique widely used when we need to excise renal tumor without excising the whole kidney. It is extremely valuable when kidney function preservation is one of our top priority. Common techniques of partial nephrectomy includes open surgery, laparoscopic surgery and robotic surgery. With the application of robotic surgery since 2008, partial nephrectomy is becoming more and more widely performed. While it is still a very invasive surgery that requires massive amount of experience, surgeons nowadays are very experienced and carry out the surgery on a daily basis. Therefore, we try to analyze what might be the variables associated with the surgical outcomes.  
  
We use PREMIER data base, which is an insurance claim-based database that provides all of the billable interventions during hospitalization.
The data does not require web scrapping and wrangling. However, most of the data we have to preprocess in order for us to use.  
For exmple, we make sought through patient medical history by looking at disease code that is billed and calculate the Charlson comorbidity index of each patient. We then categorize them into 3 categories, 0, 1, and 2. Category 0 means Charlson index 0. Category 1 means Charlson index 1 and 2. Category 2 means Charlson index 3 and above.


```{r}
library(tableone)
library(reshape2)
library(ggplot2)
library(tidyverse)
library(haven)
library(quantreg)
library(caret)
library(jpeg)
```

```{r}
df<-read_dta("cohort_nephrectomy.dta")
```

```{r}
#Clavien-Dindo post operative complication classification has 7 levels, I, II, IIIa, IIIb, IVa, IVb, V
#We dichotomize them into 0 and 1. 0 means I and II, 1 means III and above.
df<-df%>%
  transform(clavien_cat = ifelse(clavien<=1, 0,1))
```

```{r}
df$icu<-as.factor(df$icu)
df$obesity<-as.factor(df$obesity)
df$hemostatic<-as.factor(df$hemostatic)
df$hemorrhage<-as.factor(df$hemorrhage)
df$bleeding_cx<-as.factor(df$bleeding_cx)
df$surgical_cx<-as.factor(df$surgical_cx)
df$urology_cx<-as.factor(df$urology_cx)
df$vte_cx<-as.factor(df$vte_cx)
df$readmit_any<-as.factor(df$readmit_any)
df$frailty<-as.factor(df$frailty)
df$tobacco<-as.factor(df$tobacco)
df$prior_tx<-as.factor(df$prior_tx)
df$thrombus<-as.factor(df$thrombus)
df$metastasis<-as.factor(df$metastasis)
df$retro<-as.factor(df$retro)
df$race_cat<-as.factor(df$race_cat)
df$female_yes<-as.factor(df$female_yes)
df$charlson_cat<-as.factor(df$charlson_cat)
df$teaching_yes<-as.factor(df$teaching_yes)
df$urban_yes<-as.factor(df$urban_yes)
df$bed_cat<-as.factor(df$bed_cat)
df$any_comp<-as.factor(df$any_comp)
df$mortality<-as.factor(df$mortality)
df$major_comp2<-as.factor(df$major_comp2)
df$partial<-as.factor(df$partial)
df$surg_approach<-as.factor(df$surg_approach)
df$payor<-as.factor(df$payor)
df$toradol<-as.factor(df$toradol)
df$exparel<-as.factor(df$exparel)
df$clavien_cat<-as.factor(df$clavien_cat)
```

```{r}
#Create new variable: dichotomize surgeon volume and hospital volume
#if volume > national median then it is high volume
df<-df %>%
  mutate(nsv = ifelse(surg_vol > median(surg_vol), "1", "0")) %>%
  mutate(nhv = ifelse(hosp_vol > median(hosp_vol), "1", "0")) %>%
  subset(partial == "1") %>%
  subset(!is.na(AGE))
#There are 7 NA in the whole data of 55582 patients. 
#We assume that the missing is completely at random. 
#The missing percentage is very low, we perform complete case analysis. 
#It will barely affect our power. 
```

```{r}
df$nhv<-as.factor(df$nhv)
df$nsv<-as.factor(df$nsv)
```

```{r}
df1 <- df %>% dplyr::select(AGE, female_yes, race_cat, charlson_cat, payor, or_time, icu, TOT_COSTS_20, readmit_any, hemorrhage, any_comp, nsv, nhv, LOS, morph_total, clavien_cat, surg_approach, year)
#The data from PREMIER database has over 150 variables. 
#After closely examine each variable with our subject knowledge, 
#we pick these above variables to be all of the features that will be used for machine learning.
```

**Variables**  
AGE: age of the patient.  
female_yes: 1 -> female, 0 -> male  
race_cat: race of the patient. 1 -> White, 2 -> Black, 3 -> Hispanic, 4 -> others  
charlson_cat: we take the Charlson comorbidity index and collapsed it into 3 categories. 0 -> Charlson comorbidity index 0, 1 -> charlson comorbidity index 1 to 2, 2 -> Charlson comorbidity index greater or equal to 3.  
payor: the payor of each patient. 1 -> Medicare, 2-> Medicaid, 3 -> Commercial, 4 -> Self-pay, 5 -> others.
or_time: operation time, in minutes.  
icu: admission to ICU, 0 -> no, 1 -> yes.  
TOT_COSTS_20: total cost during admission, adjusted to dollar in 2020.  
readmit_any: readmission to urology ward due to surgery related cause in 30 days. 0 -> no, 1 -> yes  
henorrhage: intra-operation bleeding that require transfusion. 0 -> no, 1 -> yes.  
any_comp: any complication. 0 -> no, 1 -> yes.  
nsv: the number of partial nephrectomy the surgeon perform in a year. We categorize into 0 -> low volume, the number of partial nephrectomy performed by this surgeon is less than or equal to national median. 1 -> high volume.  
nhv: the number of partial nephrectomy the hospital perform in a year. We categorize into 0 -> low volume, the number of partial nephrectomy performed by this hospital is less than or equal to national median. 1 -> high volume.  
LOS: Length of stay, in days.  
morph_total: total amount of morphine administered for pain control, in milligrams.  
clavien_cat: we categorized Clavien-Dindo classification of postoperative complication into 0 -> Clavien-Dindo classification I or II. 1 -> Clavien-Dindo classification greater than or equal to III.  
surg_approach: surgical approach. 0 -> open surgery, 1 -> laparoscopic, 2 -> robotic surgery.



```{r} 
#create train test split at 80:20
set.seed(1990) 
index <- sample(nrow(df1), 44460) 
dftrain <- df1[index,] 
dftest <- df1[-index,] 
``` 

**Linear model**  
First, we use operation time as our surgical outcome. We try to analyze what are the variables associated with operation time. Since the operation time is continuous, we use linear model.
```{r}
#we will use cross validation in the linear model
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model <- train(or_time ~., data = dftrain, method = "lm",
               trControl = train.control)
# Summarize the results
print(model)
```

```{r}
predictions <- model %>% predict(dftest)
data.frame( R2 = R2(predictions, dftest$or_time),
            RMSE = RMSE(predictions, dftest$or_time),
            MAE = MAE(predictions, dftest$or_time))
```
We use linear model with 10 fold cross validation. The RMSE in the training set is 233.58 and RMSE in the test set is 235.46. We can see that there is no significant overfitting in this model.


**logistic regression**  
We then take a look at Clavien-Dindo category as our surgical outcome. We want to know what variable might be associated with post operative complication (Clavien category = 1).
Since the outcome is binary, we use logistic regression with cross validation.
```{r}
train_control <- trainControl(method = "cv", number = 10)

# train the model on training set
model <- train(clavien_cat ~ .,
               data = dftrain,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
summary(model)
```

```{r} 
y_hat_logistic <- predict(model, dftest, type="raw") 
cm <- confusionMatrix(y_hat_logistic, dftest$clavien_cat) 
cm$overall["Accuracy"] 
``` 

in the model we got using logistic regression with 10 fold cross validation, we can predict our test set with 84.36 percent accuracy. the model performed quite well.  
We can use these simple models as benchmark to see how machine learning techniques perform.  
  
**knn**  
We now use knn and repeat previous attempts where we use operation time and Clavien category as outcomes.

```{r}
# your code here
x<-dftrain %>% select(-or_time)
control <- trainControl(method = "cv", number = 10) 
train_knn <- train(x, dftrain[,6],  
                   method = "knn",  
                   tuneGrid = data.frame(k = seq(110,200,10)), 
                   trControl = control) 
train_knn 
``` 


```{r}
plot(train_knn)
```

After tuning our knn model we can see that when k=190 we have the smallest RMSE in the training set.
We then proceed to the test set.

```{r}
predictions1 <- train_knn %>% predict(dftest)
data.frame( R2 = R2(predictions1, dftest$or_time),
            RMSE = RMSE(predictions1, dftest$or_time),
            MAE = MAE(predictions1, dftest$or_time))
```

In our test, the RMSE is 237.64, compared to our training set 236.22, there is not much overfitting happening.\\
However, if we compare to the RMSE we get in the test set from the linear model, which is 235.46, we can see that our knn model performed not as good as linear model.

```{r}
# your code here
x1<-dftrain %>% select(-clavien_cat)
control <- trainControl(method = "cv", number = 10) 
train_knn1 <- train(x, dftrain[,16],  
                   method = "knn",  
                   tuneGrid = data.frame(k = seq(90,160,10)), 
                   trControl = control) 
train_knn1 
``` 

```{r}
plot(train_knn1)
```

```{r} 
y_hat_knn <- predict(train_knn1, dftest, type="raw") 
cm <- confusionMatrix(y_hat_knn, dftest$clavien_cat) 
cm$overall["Accuracy"] 
```

The accuracy in our training set is 76.25 percent. The accuracy in our test set is 75.65 percent. There is not much overfitting happening here. However, if we compare to our logistic regression model, which has accuracy of 84.36 percent in the test set, knn model does not perform as good as the logistic regression model.

**Random forest**  
We repeat the above tasks with random forest technique.

```{r}
library(randomForest)
```

```{r}
ortrf <- randomForest(or_time ~ ., data = dftrain, mtry = 3,
                         importance = TRUE, na.action = na.omit)
ortrf
```

```{r}
plot(ortrf)
```

```{r}
RMSE<-sqrt(min(ortrf$mse))
RMSE
```

The RMSE in the training set is 222.97

```{r} 
rfPredict <- predict(ortrf,newdata = dftest )
```

```{r} 
head(data.frame(dftest$or_time, rfPredict))

mse = mean((dftest$or_time - rfPredict)^2)
mae = caret::MAE(dftest$or_time, rfPredict)
rmse = caret::RMSE(dftest$or_time, rfPredict)

cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmse)
``` 

The RMSE in the test set is 224.52. It is bigger than the RMSE in the training set, which has RMSE of 222.97, but not by too much. We can say that there is no overfitting happening here. The RMSE in the test set is significantly smaller than the RMSE we got from both linear model with 10 fold cross validation and knn model.
We concluded that in the prediction of operation time of partial nephrectomy using the PREMIER database, the random forest method has the best performance. Linear model with 10 fold cross validation performance comes in second. The performance of knn model is the worst.

We now try the random forest method in the analysis where Clavien category is the outcome.


```{r}
clcrf <- randomForest(clavien_cat ~ ., data = dftrain, mtry = 3,
                         importance = TRUE, na.action = na.omit)
clcrf
```

```{r}
plot(clcrf)
```

```{r} 
y_hat_rfc <- predict(clcrf,newdata = dftest ) 
cm <- confusionMatrix(y_hat_rfc, dftest$clavien_cat) 
cm$overall["Accuracy"] 
```

In the training set, we get the out of bag error rate 16.1 percent. the accuracy is 83.9 percent.
In the test set, our model has an accuracy of 84.5 percent. There is no overfitting happening here.
In conclusion we can see that in the task of predicting post operative complication Clavien-Dindo category by using the PREMIER data base, the performance of random forest is slightly better than logistic regression with 10 fold cross validation. The knn model performed the worst.

**Conclusion**
We developed prediction models for operation time and post operative complication by using PREMIER data base.
We created 3 different models for these 2 different outcomes respectively. The models performed well and did not overfit. The model is potentially useful in the surgeon patient communication and preoperative patient education.

